## 介绍⭐

一个月完成

真实可上线

api接口文档开发

五大模块：用户，视频，审核，后台管理，资源保护





## Apifox文档

接口管理测试

https://apifox.com/apidoc/shared-50452d70-b0fb-4f24-8057-dcca465e8380



## 技术选型 需求分析

 **前端 - 小伙伴**

 负责设计实现前端所有模块的页面和交互 

 **技术选型：**

vite + vue3 + axios + pina + router + vuetifyUI + videojs + 七牛云sdk

 **后端 - 徐昊** 

负责设计实现后端所有模块

**技术选型：**

Jdk1.8 + SpringBoot2.7 + MyBatis + MySql5.7 + Redis + 七牛云存储 + 七牛云审核 + 七牛云转码 + 七牛云回源鉴权



## 为什么不用SpringCloud

真实可上线

如果用微服务就要买好多个服务器，把每个服务分开部署，工作量过大

如果部署在一台服务器上，那么你微服务的意义是什么？没意义



## Redis里放什么

用户模型

关注

粉丝

热度排行榜

热门视频

分类视频

标签视频

用户浏览记录

用户去重布隆过滤器

用户发件箱，收件箱



## 表设计⭐

base entity 在应用层层面 封装统一字段 `extends BaseEntity`

创建时间

修改时间

逻辑删除

...



所有 id 全用的 **bigint(20)** 来存



### 视频模块

#### 视频表 video

```sql
CREATE TABLE `video` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `yv` varchar(255) DEFAULT NULL,
  `title` varchar(255) NOT NULL,
  `description` varchar(255) DEFAULT NULL,
  `url` varchar(255) DEFAULT NULL,
  `user_id` bigint(20) NOT NULL,
  `type_id` bigint(20) NOT NULL COMMENT '类别id',
  `open` tinyint(1) NOT NULL DEFAULT '0' COMMENT '公开/私密，0：公开，1：私密，默认为0',
  `cover` varchar(255) DEFAULT NULL,
  `audit_status` int(11) NOT NULL,
  `msg` varchar(255) DEFAULT NULL,
  `audit_queue_status` tinyint(4) NOT NULL DEFAULT '0' COMMENT '审核队列状态',
  `start_count` bigint(20) DEFAULT '0',
  `share_count` bigint(20) DEFAULT '0',
  `history_count` bigint(20) DEFAULT '0',
  `favorites_count` bigint(20) DEFAULT '0',
  `label_names` varchar(255) DEFAULT NULL,
  `duration` varchar(20) DEFAULT NULL,
  `is_deleted` tinyint(1) DEFAULT '0' COMMENT '逻辑删除，0：未删除，1：删除，默认为0',
  `gmt_created` datetime DEFAULT NULL,
  `gmt_updated` datetime DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=4832 DEFAULT CHARSET=utf8mb4 ROW_FORMAT=DYNAMIC;
```

id

标题

描述

分类

视频源：url，文件表 id，通过文件表id获取key

封面：文件表 id

标签

发布者（用户id

状态 open 公开/私密，0：公开，1：私密，默认为0

审核状态：通过，审核中，PASS

审核信息：正常，null，审核违规信息

点赞次数

浏览次数

分享次数

收藏次数

创建时间

修改时间

#### 分类表 type

```sql
CREATE TABLE `type` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `name` varchar(255) DEFAULT NULL,
  `description` varchar(255) DEFAULT NULL,
  `open` tinyint(1) DEFAULT '0',
  `icon` varchar(255) DEFAULT NULL,
  `sort` int(11) DEFAULT '0',
  `label_names` varchar(255) DEFAULT NULL,
  `is_deleted` tinyint(1) DEFAULT '0',
  `gmt_created` datetime DEFAULT NULL,
  `gmt_updated` datetime DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=23 DEFAULT CHARSET=utf8mb4 ROW_FORMAT=DYNAMIC;
```

#### 视频点赞表 video_star

视频id

用户id

#### 视频分享表 video_share

视频id

用户id（不登录也能分享，该字段可能为空）

IP地址

> 视频收藏表 favorite_video 划分到**用户模块**，不在视频模块展开讲
>
> ps：用户和视频的**交叉**模块



### 用户模块

#### 用户表 user

id

昵称

邮箱

密码

个人介绍描述

性别

默认收藏夹id

#### 权限表 permission

id

权限名

菜单名

排序

...

#### 角色表 role

id

角色名

备注 remark

#### 权限角色中间表 role_permission

id

权限id

角色id

#### 用户角色中间表 user_role

id

用户id

角色id



#### 浏览记录（没有表）

量大，数据库压力大，而且不是很重要，不存MySQL，存Redis Zset ttl 1或2个星期

但是浏览次数要加在视频表中的浏览次数字段 history_count 上 

#### 搜索记录（没有表）

量大，数据库压力大，而且不是很重要，不存MySQL，存Redis Zset ttl



#### 收藏夹表 favorites

id

名称

介绍

收藏视频数量（冗余，可以用中间表计算）**ps：实际数据库表中并没有这个字段**

创建者

创建时间

#### 收藏夹视频中间表 favorites_video

收藏夹id

视频id

用户id（可以没有，讲课的时候没提，但是实际数据库里有）

#### 订阅分类表 user_subscribe

id

用户id

分类id  -->  感兴趣分类id

#### 关注粉丝 follow

id

用户id（粉丝）

关注id follow_id（大V）



### 审核模块

#### 审核表

大部分字段都存在 **video** 表里了

存放审核的一系列 --> 图片，视频，文本

类型：当前数据审核的是图片？视频？文本？

- 文本，图片审核很快，审核不通过没有消息发送，只是显示违规；

- 视频审核很慢，异步处理



### 资源保护模块（文件表）

#### 文件表

id

file_key(七牛oss)

文件类型

文件大小

视频/图片 format

ps：视频时长 duration（不确定是不是这个字段）



### 系统配置表

**审核力度**：根据七牛云审核得分score

自动通过 --> 0.8-0.9

需要人工审核 --> 0.7-0.8

PASS --> 0.0-0.7











## 架构设计

![架构设计](assets/架构设计.jpg)





## 功能模块：用户，视频，审核中台，**后台管理界面**:point_down:

抓住两大主线：**用户**，**视频**

![输入图片说明](image/%E5%8A%9F%E8%83%BD%E6%A8%A1%E5%9D%97.png)

## 用户

### 个人信息

CRUD

### 关注/粉丝

一般短视频平台量关注量很大，用Redis

用户可对TA人进行关注/粉丝，关注后即可在关注推送TA人的视频，取关后取消推送

### 用户收藏夹

CRUD

用户可对视频进行收藏管理

**注：用户注册后生成默认收藏夹，不可删除**

### 浏览记录

有序的，Redis Zset

用户所刷的视频将保存在此为5天

### 订阅分类

CRUD

用户可订阅感兴趣的分类，后续在推荐页中可被推荐更多相似视频

## 视频

### 上传视频

发布视频引出**关注推送**，**分类推送**。

异步处理：使用多线程，而不使用MQ。

why：只有一台服务器，把MQ和项目都部署在一台服务器上，没用。引入技术框架，维护MQ也是一个成本。

![输入图片说明](image/%E4%B8%8A%E4%BC%A0%E8%A7%86%E9%A2%91.png)


### 播放视频

视频存储在云端,需要被保护,用到了七牛云的回源鉴权

 **流程** 

1.获取资源请求服务器

2.服务器判断当前请求referer是否包含在系统白名单内

3.1.不包含 403 END

3.2.包含

4.生成UUID TTL:8秒 放入caffeine中并且拼接url

5.重定向七牛云资源

6.七牛云转发鉴权服务

7.鉴权服务校验是否合法

![输入图片说明](image/%E8%8E%B7%E5%8F%96%E8%B5%84%E6%BA%90%E6%B5%81%E7%A8%8B%E5%9B%BE.png)

### 点赞

兴趣推送中用户模型的更改

异步解耦



### 兴趣推送

**用户**

用户在订阅分类后，可进行相似推送

**游客**

随机分类推送

 **设计** 


每个分类拥有默认标签，每个视频可拥有最多5个标签，用户订阅分类后，实际是订阅了分类后的标签，将标签概率分布，后续兴趣推送则获取用户的标签，进行随机概率获取推送视频。浏览停留时长,点赞,收藏行为将对对应的标签概率进行增减

**系统标签库**

数据结构：


```
Set ttl: -1

key: system:stock: + 标签id

value: 视频ids
```


**用户模型库**

数据结构:


```
Map ttl: -1

key: user:model: + 用户id
 
value : 

key: 标签id

value: 概率
```

**流程**

1.用户订阅分类后取出分类后的默认标签，标签数/100 得到同等概率，存到用户模型库

2.用户在 **视频停留时长/点赞/收藏** 将对该视频下的所有标签拿到 **模型库中进行增长/缩减** ，每次增长后需要同等比缩小概率，**防止概率膨胀**

3.推送视频时获取用户的模型库，将其组装为数组，下标为label Id，随机数取数组长度，获取videoId，并和浏览记录去重，再根据用户性别标签获取视频，封装数据后返回，**这里可能会推送很少的视频，不要紧**

4.**前端拉取推荐视频根据上一次推送视频集合的阈值拉取,例如集合扩容机制** 

推送流程图

![输入图片说明](image/%E5%85%B4%E8%B6%A3%E6%8E%A8%E9%80%81.png)



### 热门推送

数据结构：


```
Set ttl: 3天

key: hot:video

value: hotVideo
```

**设计** 

每隔3个小时 **切片快速分页扫描全表** ，每个视频计算热度值后和系统配置表的热度值做比对，小于则放入热门视频

**推送：**

热门视频属于随机推送且下拉获取新视频，不需要分页，如果出现相似视频则说明是**数据量不够大**



### 热度排行榜

根据视频的热度进行排行

 **设计** 

**热度 = 权重 和时差的计算**

权重 = 点赞,浏览,分享,收藏对应的占比例

时差：当前时间 - 视频发布时间的差值

A视频24小时内点赞到了1W，B视频1小时内点赞到了1W，则说明B视频热度更高

这里可以采用半衰期公式计算热度

总结可以理解为  **当前时间 - 视频发布时间 差值为x ，x越小y越大，x越大y越小 后 对应的权重 得到热度** 

数据结构：


```
Zset ttl: -1

key：hot:rank  

value: videoId 

socore: 热度  
```


每隔1个小时切片快速分页扫描全表，**每个视频计算热度值后放入有界的小根堆,遍历完成再放入Redis -> TopK问题**



### 关注推送

推送关注人发送的视频 -> feed流

 **设计** 

用户拥有**发件箱**和**收件箱**

**发件箱**

用户所发布的视频存储在发件箱

数据结构：


```
Zset ttl:-1

key: out:follow:feed: + 用户id

value: 视频id

Score: 视频发布时间 
```


**收件箱**

存储用户关注人的视频

数据结构：

```
Zset ttl:5天

key：in:follow:feed: + 用户id

value: 视频id

score: 视频发布时间 
```

**流程**

1.用户发布视频后，将视频异步发送到发件箱

2.用户上线后异步获取关注流：

  2.1关注流为空，则拉取关注人7天之内的视频

  2.2不为空，则拉取收件箱最新视频的时间 - 当前时间内关注人的视频并存入收件箱

3.用户删除视频将异步删除发件箱视频，以及粉丝内的收件箱视频

4.用户拉取关注流根据滚动分页获取

推拉模式的选择是需要根据当前项目的数据体量决定的。当前项目体量不大，选择拉模式且设置ttl，过滤不活跃粉丝


 **收件箱初始化**

![输入图片说明](image/%E6%94%B6%E4%BB%B6%E7%AE%B1%E5%88%9D%E5%A7%8B%E5%8C%96.png)


 **拉取关注视频**

![输入图片说明](image/%E6%8B%89%E5%8F%96%E6%94%B6%E4%BB%B6%E7%AE%B1%E8%A7%86%E9%A2%91.png)



### 分类推送

根据分类随机推送视频，不需要分页，不需要去重，因数据量少

**一切的设计实现都要考虑当前项目的因素落地**



## 审核中台

审核中台可自定义放行比例以及设置是否开启审核

![输入图片说明](image/%E5%90%8E%E5%8F%B0%E5%AE%A1%E6%A0%B8%E7%95%8C%E9%9D%A2.png)

 **设计** 

AuditService: 规定审核标准，规定入参返回值 <T,R>

AbstractAuditService: 封装统一逻辑 : 比较得分，获取消息，返回对应审核状态(策略模式)

ImageAuditService: 图片审核

TextAuditService: 内容审核

VideoAuditService: 视频审核

![输入图片说明](image/%E5%AE%A1%E6%A0%B8%E4%B8%AD%E5%8F%B0%E8%AE%BE%E8%AE%A1.png)

VideoPublishAuditServiceImpl: 发布视频审核设计

![输入图片说明](image/%E9%A1%B9%E7%9B%AE%E5%AE%A1%E6%A0%B8%E8%90%BD%E5%9C%B0%E5%AE%9E%E7%8E%B0.png)





## 后台管理界面

### 权限模块

使用RBAC实现权限模块,超级管理员可自行分配角色

![输入图片说明](image/%E5%90%8E%E5%8F%B0%E6%9D%83%E9%99%90%E7%95%8C%E9%9D%A2.png)

### 系统配置

系统配置中配置了审核力度、审核开关、热门视频热度限制、白名单

![输入图片说明](image/%E5%90%8E%E5%8F%B0%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE.png)


### 视频模块

可对视频进行下架审核处理

![输入图片说明](image/%E5%90%8E%E5%8F%B0%E8%A7%86%E9%A2%91.png)

### 分类模块

可管理首页的分类

![输入图片说明](image/%E5%90%8E%E5%8F%B0%E5%88%86%E7%B1%BB.png)





## 改进优化:point_up:

### 架构改进

当前项目为单体架构,为什么没考虑微服务如下:

1.个人认为无论是实际项目还是比赛,不以 **实际解决问题出发而引入某些技术就是在炫技** ,无非是增加了开发成本,运维成本等。例如：不考虑用户体量的场景下就做分库表,引入ES做搜索,MQ做异步解耦

架构改进如下:

 **1.视频服务** 



 **2.用户服务** 


 **3.评论服务** 

评论服务抽出来是考虑到后续产品会出 **动态** 功能,因此将评论服务抽出来


 **4.点赞收藏浏览分享服务** 

该服务考虑到后续可能会有对视频,动态,评论等有操作

 **5.审核中台** 


 **6.鉴权服务** 

用于对资源的保护

 **7.网关** 

路由请求转发



### @Async 改进 MQ

在项目中大量使用了线程解耦,实际引入MQ



### 分片存储视频

项目中Redis有一个分类库,用于存储所有的视频达到随机推送视频，且ttl为-1,项目中未做分片,会造成大key

 **分片设计** 

1.每个分类维护一个分片id,且限制分片id最大存储X条数据。 数据结构String key: 分类id  value: 自增id

2.系统启动时将分片id存储本地缓存

3.存储视频时,先判断对应分类中的数量是否达到限制

3.1.未达到 - 跳到4
3.2.达到限制  - 将本地缓存自增1,异步修改Redis对应分类id   跳到4

4.获取本地缓存对应分类id

5.取对应id内的随机数，达到分片获取数据。如果想避免数据倾斜(随机数很旧,获取视频不是最新),可指定具体id进行获取数据



### Feed流

当前项目中是以拉模式实现,用户上线后拉取内容且设置ttl。这里应该做成推拉模式，用户发布一个视频后，推送到活跃用户的收件箱

这里的设计是考虑了项目体量决定



### 对象存储

对象存储在项目中是将和资源相关暴露给了前端,实际该尽可能减少暴露

 **设计** 

1.设计文件表,用于管理所有的文件

2.视频表关联文件表, file_id = file_key

3.获取资源时根据file_id从文件表中查询file_key进行重定向

4.鉴权...



### 审核中台

在项目中审核的设计为嵌入式服务,应该将其抽出改为单独服务,并且提供更多的信息



### 分享

分享未做短链接,实际应该做短链接处理,存储视频信息,用户信息等





## 防止重复提交

> **业务背景**
>
> 在发布视频时，如果遇到重复提交，则会发布多条视频，同理，其他的评论等功能也是一样

需要注意的是，**防止重复提交**和**幂等性**是俩个概念

- 防止重复提交：指的是在前端，因为服务器网络波动的原因，未能及时返回响应，用户误以为是卡了，从而又点击了一次提交，因此提交了两次请求
- 幂等性：指的是在接口层面，可能因为多方面的原因(MQ重复消费)重复发送了数据，未能正确的处理，从而导致重复消费了多条数据。

**解决方案：**
**前端：**
前端在提交表单后，触发 loading 动画，锁住提交按钮，并且提示用户：`正在上传中`/。上传完返回消息后取消动画
**后端：**
**方案1**
前端浏览到发送表单的页面后，生成一个唯一id，发送请求的时候一并发送给后端，后端将唯一id判断并且存储，如果已存储，则说明已经请求过，不予理会，否则将唯一id进行存储。

这里涉及到并发问题，因为涉及到了先查询再存储的行为。大家可以理解为秒杀场景的查库存扣订单。因此这里用秒杀解决方案解决。

**单体解决方案：**
利用 synchronized 锁唯一id即可

**分布式解决方案：**
可通过 redis 的自增实现
数据结构：string，key：唯一id，value：number
自增后，查看返回的 number 是否大于 2，如果大于 2 说明已经请求过了

该解决方案依靠了 redis 天然的串行化。传统的方式是 查，改，在查这一步导致的线程之间不可见，而 redis 多个线程改了后是可见的

**方案2**
通过前端发送的请求参数获取哈希值，锁住哈希值即可，但是该方案可能会哈希冲突

实际采用：
前端虽然可控制，但是用户可以通过 http 工具进行访问，因此无法限制，后端的两个方案，建议使用方案1，因为方案2可能会有哈希冲突





## 技术选型

如何根据需求分析所需要的技术是在开发前重要的一环





## 部署⭐

### 服务器信息

- CPU - 2核 内存 - 2GB
- 系统盘 - SSD云硬盘 50GB
- OpenCloudOS 9
- 公网IP：43.143.165.213
- http://43.143.165.213:面板端口/tencentcloud（面板端口默认8888）



### 流程

本地部署没问题

注意：

1. pom.xml没有要改的，完全可以直接运行
2. application.yml
3. MySQL，Redis，七牛oss，邮箱SMTP，QiNiuConfig类

<img src="assets/53f351b3b8d23038fa5486467375856.png" alt="53f351b3b8d23038fa5486467375856" style="zoom: 67%;" />



### 部署面试输出

- 使用宝塔面板



### 七牛云测试域名 缓存问题⭐

遇到**未上传封面**而使用的是**视频截帧截取封面**缺导致视频加载不出来，可能是因为**七牛云测试域名缓存**的问题

因为七牛云测试域名的原因会缓存，而导致这个问题的发生其实是可控的，问题复现：

1. 上传视频

2. 不上传视频封面

**无封面**的情况下会**取视频的第一帧**，而**第一帧是从源视频 url 上进行获取**，在没有播放视频前是先获取的图片地址，因此这个**url（视频）会被缓存为 图片**。那设想：如果在访问前先访问了这个视频，这个缓存问题其实也就没了。（但是不会出现这种情况，除非手动，你可以试试，上传后在七牛云直接打开、而不是通过项目获取视频列表，那这个时候就已经晚了）







## 注册 & 登录 & 找回密码 & 校验身份 - 视频

思考：

- 为什么涉及第三方服务需要用到图形验证码？
- 找回密码流程能否自主思考？
- ThreadLocal解决了什么问题？[ThreadLocal](https://code.xhyovo.cn/article/view?articleId=109)

>  问题
>
> 在使用ThreadLocal时候，向外暴漏的接口必须强制在形参中传参，禁止在方法内直接凭空 ThreadLocal.get()。不然会导致方法意义不明确。在项目中未能遵循。
> 例如：
> VideoServicer#deleteVideo(Long id) 中只写了 videoId，实际应该把 userId 也一并传入。
> 在 Controller 层面不用理会，因为 Controller 不会向外暴漏，而是交给 web 框架调用



### 涉及第三方服务需要用到图形验证码

防人机恶意刷

#### 如果不用验证码使用限流

- 限制邮箱短时间内只能发几次

  但是可以换邮箱继续刷

- 限制IP地址

  可能一整个网段用同一个IP，封的话也封不好



#### 图形验证码

uuid，code保存在redis中，加一个ttl





#### 邮箱数字验证码

code保存在Redis中，加一个ttl



### 注册

![1734248770062](assets/注册流程.png)

![1734412675421](assets/1734412675421.png)



### 登录

jwt



### 忘记密码

很像注册，还需要图形验证码和邮箱验证码



### ThreadLocal解决了什么问题？

### 请问AdminInterceptor类中有必要重写HandlerIntercepter接口的afterCompletion方法吗？就是后端业务完成后，最后执行这个方法，在这个方法中调用UserHolder.clear()方法，清除threadlocal内存，避免内存泄漏

> HandlerIntercepter的afterCompletion是在什么时候执行？
>
> ThreadLocal是否必须要调用clear方法？为什么？

1、HandlerInterceptor的afterCompletion方法是在请求处理完成之后执行的。这包括渲染视图之后，也就是说在请求的整个处理流程结束时执行。在请求处理结束后，不管有没有抛出异常，都要执行afterCompletion方法。

2、在 HandlerInterceptor 的 preHandle 方法中获取用户信息并存入 ThreadLocal 中时，为了确保线程安全和避免内存泄漏，应该在 afterCompletion 方法中清理 ThreadLocal。

**上面这个没有答复：ThreadLocal是否必须要调用clear方法？为什么？**

如果是用线程池维护的一组线程，则必须要进行clear()。因为线程会被复用，这些变量可能不会被垃圾回收，不主动清除的话，会造成ThreadLocal一直保存在内存中，占用大量内存，不同用户请求之间可能会共享线程，造成状态污染。而这里如果不是用的线程池技术，一个用户分配一个新的线程的话，当线程执行完HandlerIntercepter的afterCompletion方法，当前用户请求的线程就要死亡了，线程栈中的所有变量，包括ThreadLocal变量，都应该被垃圾回收，因为没有其他对象持有它们的引用。JVM的垃圾收集器会自动清理这些变量。所以不调用clear方法也可以

**很好，你已经明白了，我再考考你，我们这个地方需要用到ThreadLocal.clear()吗？为什么？**

我又查了这个：当 ThreadLocal 实例被静态变量持有时，即使某个线程结束了，该线程中的 ThreadLocal 实例及其对应的值虽然不再被访问，但由于静态变量持有了 ThreadLocal 实例的引用，垃圾回收器无法回收这些实例。只有在显式调用 remove() 方法后，才会解除静态变量对 ThreadLocal 实例的引用，使得 ThreadLocal 实例及其关联的值能够被垃圾回收器回收。

**你查错方向了，你现在应该去考虑请求是否是一个线程池？如果是，那就得 clear**

**springmvc底层是servlet，都是依赖tomcat，你猜猜tomat有没有用到线程池？一个请求对应一条线程，如果没有线程池会怎么样?**

Tomcat 是一个基于 Java 的 Web 服务器和 Servlet 容器，它在处理请求时确实使用了线程池来管理和复用线程资源。这是为了提高并发处理能力和资源利用率。
如果 Tomcat 没有使用线程池，而是为每个请求创建新的线程，可能会导致以下问题：

资源耗尽：每个请求都创建一个新线程会消耗大量的系统资源，特别是在高并发情况下，系统可能很快耗尽可用的线程数和内存，导致系统性能急剧下降甚至崩溃。
性能问题：频繁创建和销毁线程会增加系统的开销，包括线程创建和上下文切换的成本，这会降低系统的整体性能和响应速度。
不可控的并发：没有线程池管理，无法限制并发请求的数量和资源消耗，可能会导致系统无法有效地控制和调度请求。

**那你说现在需要用到 ThreadLocal.clear() 吗，答案是不是出来了呢**

> ThreadLocal
>
> Thread --> ThreadLocalMap（hash表） --> (弱引用)ThreadLocal（Key），Object（Value）









## CDN和OSS

卡顿并不是一味的找是否是站点的原因，这跟遇到代码bug一样，你会下意识的说是代码出问题了吗？因为我们是一个网站，如果一个人卡，则所有人都会卡。我这里引用我让其他人排查的原因：

 

你问得好好，我下次注意提问方式。我挂了Clash代理，新加坡换成美国节点好点了。查了下你说的：
**卡顿**：这里的“卡”指的是在观看视频时，视频加载速度慢、画面停滞，或者音视频不同步的情况。

- **倍速卡**：如果用户选择了倍速播放（比如1.5倍速、2倍速），可能卡顿会更明显，因为倍速播放需要更多带宽和更快的数据加载。
- **代理卡**：用户可能使用了网络代理（比如VPN）来连接服务器，如果代理连接不稳定或者延迟较高，视频播放也会卡顿。
- **节点**：代理节点指的是VPN提供的服务器位置，通常是全球不同地区的服务器节点。不同节点可能有不同的网络状况。

CDN和OSS解释：

1. **CDN (内容分发网络)**：CDN是为了加速内容传输而设计的网络服务，它通过在全球分布多个服务器节点，来缓存网站的内容（如图片、视频等），让用户可以从离自己最近的节点获取资源，从而加快访问速度，减少延迟。- **命中缓存**：指用户请求的内容（比如视频）已经被缓存到CDN的节点服务器中，用户可以直接从该节点获取数据，速度很快，基本不会卡顿。
2. **OSS (对象存储服务)**：OSS是一种存储大量非结构化数据的服务，通常用来存储图片、视频、备份等大文件。当用户请求的资源没有被缓存到CDN节点时，CDN会去原始存储（即OSS）获取数据。- **走OSS**：如果CDN没有缓存用户请求的资源，系统会从远程的OSS服务器拉取该资源。由于OSS通常位于集中式的服务器，距离用户较远，可能会导致加载时间增加，出现卡顿现象。

 

- 视频资源通过CDN分发，如果用户的请求命中了CDN缓存，视频会快速加载，不会出现卡顿。
- 但如果用户请求的资源没有被缓存到CDN，就需要通过OSS获取数据，这个过程可能因为距离较远或其他因素导致视频卡顿。
- 后续在同一个节点上，当该视频已经被缓存到CDN时，用户再请求时就不会再卡了，因为CDN已经缓存了该资源。

> ps：大厂网站（比如b站）做的不卡的原因：很有可能是一个视频在同一个CDN区域内会被好多人看，你看到的视频早就被缓存到CDN里了，而不需要每一个视频都走OSS







## 基础架构 - 资源：oss存储

> **业务背景**
>
> 在网站中，必然会涉及到图片，视频等资源的上传，如何保存，如何上传，如何管理成为了一个问题

资源保存方案：
**1.存储在服务器**
存储在服务器，需要**自建文件平台**。例如：MinIO。并且上传下载资源需要考虑**存储空间**以及**流量带宽**的问题，而个体用户来说的话。我个人是服务器 2c4g，硬盘：60g，带宽：5m，流量：500g/月。 硬盘空间并没有很大，但是最主要的是服务器的时长，如果是云服务器，考虑到续费，续费会很贵。
因此资源存储在服务器上需要考虑：

- 自建平台
- 考虑云服务器续费问题

**2.存储在云上**
各个云厂商都会提供对象存储服务，这里的对象指的就是资源。将资源保存在云上，是一个很好的选择。不用考虑数据迁移。但是需要考虑容量以及流量等成本。对于各个厂商的对象存储服务来说，容量不值钱，流量才值钱，在之前出过的文章也有说明 oss 的费用，这里大家可以去看看：[社区 (xhyovo.cn)](https://code.xhyovo.cn/article/view?articleId=145)

因此**保存在云上**成了目前最好的选择。



**访问资源的流程图：**

![1734424713532](assets/访问资源流程.png)

七牛云的回源鉴权流程：

<img src="assets/七牛云回源鉴权流程.png" alt="七牛云回源鉴权流程" style="zoom: 50%;" />





**oss 签名url**
访问 oss 资源会通过 url 进行访问，而为了保证 url 防刷，必须使用加密 url，保证 url 的安全性。

**操作行为：**
**上传**
上传分为前端、后端、签名上传三种方式

- 前端上传则会把密钥暴漏在前端，这是不可能的
- 后端上传则在上传资源的时候还需要再走一遍服务器流量：前端上传-服务器-服务器上传 oss，这个策略能用，但是也不太好
- **签名上传**则在上传资源时先找后端要签名，前端通过签名上传资源：前端上传-服务器要签名-前端上传，这个策略是最好的，因为上传直接到 oss。还可以配置**回调函数callback**，自己处理上传后的逻辑

**下载**

下载资源也就是获取资源在对象存储中的 url ，url 如果不加以设置(配置防盗，签名，ttl等行为)则会被盗刷流量。

**管理**
虽然对象存储服务可以检测资源上传记录，但是为了更好的检测用户上传资源的记录，**自建一个资源表**，用于记录用户资源上传的记录，保存资源元信息（资源主键，资源大小，资源类型，资源url，上传者，上传时间等信息）。在原表中保存资源的 url 改为保存为资源表主键 id，在获取资源的时候需要通过资源主键获取资源 url，重定向到对应的云对象提供方

### 资源保护（下载）

**防止其他网站引用我们的资源来刷流量**

因资源都保存到对象存储中，访问资源是通过 url 进行访问。并且每次访问都会消耗对象存储的流量，如果不加以对资源的保护，则会消耗对象存储的流量，也就是金钱。

资源的保护分为以下几种：

- 防盗链 referer：http的referer header
  防盗链是指在对应的云厂商中配置对应的防盗链配置，一般都是浏览器中的 referer，如果配置的 referer 和浏览器发出的 referer 不一致，说明资源的发出不是来着本网站，而是其他网站(资源被盗)。但是需要注意的是 referer 是可以被伪造的
- 签名 URL
  通过加密算法可以对资源进行保护，可以限制 ttl
- 流量监控
  云厂商会自带监控行为，合理配置号对应的流量监控，到达阈值触发告警行为
- 限流
  每个用户访问资源进行 ip 的限流，每个 ip 限制每分组最多访问最大次数的资源

 

> 在本系统中采用的是七牛云的**回源鉴权**，原理是在请求资源之前，判断 **referer** 是否和配置的一致，如果一致则在 url 上携带生成的 uuid（一个资源对应一个uuid），抵达七牛云资源后又会回调接口校验 uuid 是否存在。





## 访问控制：七牛云

### referer防盗链

使用referer header

### 时间戳防盗链

时间戳防盗链可以通过对时间有关的字符串进行签名，将时间、签名信息通过一定的方式传递给 CDN 边缘节点服务器进行鉴权，从而正确响应合法请求、拒绝非法请求。相比于 referer 防盗链，时间戳防盗链的安全性更强。

#### 原理说明

时间戳防盗链的目的是使得每个请求的 url 都具有一定的 **“时效性”**，所以 url 本身需要携带过期时间相关的信息，同时还需要确保这个过期时间不能被恶意修改，因此采用 md5 算法，将 **key、过期时间、文件路径**等信息进行加密得到签名加入 url，并在 CDN 节点进行验证。

#### 鉴权过程

整个时间戳防盗链的实现需要以下几个部分配合：

- 客户端：负责发送原始请求给业务服务器以及发送带时间戳加密的 url 给 CDN 节点进行验证；
- 源站业务服务器：根据约定的[算法](https://developer.qiniu.com/fusion/3841/timestamp-hotlinking-prevention-fusion#3)生成带签名参数的 url 返回给客户端；
- CDN 节点：负责对客户端进行时间、签名校验。

主要分为以下几个步骤：

<img src="assets/时间戳防盗链.png" alt="时间戳防盗链" style="zoom:67%;" />

（1）用户管理员在七牛 [CDN 控制台](https://portal.qiniu.com/cdn/domain) 配置 key（用于加密 url 的密文） ，并将 key配置进业务服务器。
（2）当客户端请求资源时，将原始 url 发送至业务服务器。
（3）业务服务器根据[计算逻辑](https://developer.qiniu.com/fusion/3841/timestamp-hotlinking-prevention-fusion#3)，将带有时间戳签名的 url 返回至客户端。
（4）客户端使用带有时间戳签名的 url 请求资源。
（5）CDN 检查 url 签名的合法性。

**注意：若同时配置了 referer 防盗链、IP 黑白名单、时间戳防盗链，有一项不满足条件，即为不通过，响应 403 。**

![时间戳防盗链算法](assets/时间戳防盗链算法.png)



### 回源鉴权⭐

**通过uuid实现回源鉴权**

为了什么？
**你想要这个视频，必须经过我的后端代码，必须经过我的鉴权服务器，不允许不经过我的代码，直接拿图片**

可以说**回源鉴权是我们自己实现的**，因为 **referer防盗链** 和 **鉴权回调函数** 都是我们自己写的

<img src="assets/七牛云回源鉴权流程.png" alt="七牛云回源鉴权流程" style="zoom:60%;" />

如上图所示流程：

1. 用户请求到 CDN 节点；
2. CDN 节点将用户请求原封不动的转发给鉴权服务器；
3. **鉴权服务器（我的后端就是鉴权服务器）**根据请求头中的鉴权参数等给出鉴权结果；
4. CDN 节点未命中向源站获取文件；
5. 源站根据请求响应内容给 CDN ，同时 CDN 缓存内容；
6. CDN 节点根据鉴权结果将内容响应给最终用户；







## 基础架构 - 审核：调用第三方API

只要涉及到用户的发布内容到平台，都要涉及审核：资源（头像，图片，视频等），文本（昵称，个性签名，评论，博客等）...

发http请求给七牛云，带上视频url，ak，sk... 然后七牛云返回一个响应，没有SDK

### 内容输出⭐

1. 文本（前缀树，ac自动机），图片，视频审核（做不了），所以调用七牛云审核API
2. 七牛云审核不是一套SDK，是发送一个http请求，然后等待响应。文本，图片直接等待，视频审核时间比较久，设置回调函数，等待回调
3. 根据文本，图片，视频不同走不同分支，接收响应json结果：如果 result.suggest 为 **pass** 则直接通过，**block** 违规封禁，**review** 需要走**自主控制的审核**力度
4. 后台根据 suggest，lable，**score 自主控制审核力度**，参考b站充电专享视频，抖音付费直播视频可以把审核力度相对降低（允许用户擦边）（ps：虽然项目不挣钱，但是体现了有思考）



### 自主控制审核力度（review，非normal）

**suggest为review，lable不是normal（可能为很多），其余情况直接通过，以下我们把score当作一个分数线，不把他当置信度了**

是否开启审核

是否是付费视频：决定审核力度，因为没有盈利所以写成了可以任意在后台修改所有视频的审核力度，总之能讲出修改审核力度（score分数线）即可

**图片鉴黄 pulp**：normal 正常图片，无色情内容；sexy 性感图片；pulp 色情图片

**图片鉴暴恐 terror**：...

**图片敏感人物识别 politician**：...

#### 文本

只有一个 score，只需要一个置信度

#### 图片和视频

有黄，暴，政三个 lable 和 score，normal 不管，非normal 的 score为分数线

一共三个红线，只要有一个低于红线则封禁

视频截帧好多下只要有一个不合格则封禁




### 文本审核：前缀树（AC自动机加强一下）

利用前缀树可以实现敏感词过滤。
例如（以下图片来自于网络）：
文本是小明在赌场卖毒品...
利用前缀树的特性，遍历内容中的每个字符，找到前缀树是否存在该字符，如果存在则向下遍历，如果找到底部，说明是敏感词，则替换。

<img src="assets/前缀树.png" alt="前缀树" style="zoom: 50%;" />

### 资源审核（图片，视频）

视频的审核也是基于图片，每个几秒**截帧**一次进行图片审核。因此最终**都是基于图片的审核**，对于图片的审核涉及到**深度学习探测图片，OCR识别图片中的文本**。

因以上的两种审核独自实现太困难（文本审核需要大量数据，图片审核需要自主搭建）。因此一般都会**采用第三方的审核 API** 进行审核。

在该项目中的审核**可自主管控审核力度**，可将一些擦边视频放行。实现方式是调用了七牛云的审核 API，其中会返回 **score置信区间 ，score 区间为 0.1-0.99 ，分值越高，说明视频正常**。因此我们使用采用该分值实现了：审核通过、人工审核、PASS。这三个策略，也分别对应了三个不同的分值区间。



### 七牛API是http请求

**文本，图片**直接返回响应

**视频**检查以后有回调地址，使用回调函数处理后续逻辑



### 流程

通过 VideoPublishAuditServiceImpl: 发布视频审核设计，异步调用

审核中台可自定义放行比例以及设置是否开启审核

![输入图片说明](image/%E5%90%8E%E5%8F%B0%E5%AE%A1%E6%A0%B8%E7%95%8C%E9%9D%A2.png)

 **设计** 

AuditService: 规定审核标准，规定入参返回值 <T,R>

AbstractAuditService: 封装统一逻辑 : 比较得分，获取消息，返回对应审核状态(策略模式)

ImageAuditService: 图片审核

TextAuditService: 内容审核

VideoAuditService: 视频审核

![输入图片说明](image/%E5%AE%A1%E6%A0%B8%E4%B8%AD%E5%8F%B0%E8%AE%BE%E8%AE%A1.png)

VideoPublishAuditServiceImpl: 发布视频审核设计

![输入图片说明](image/%E9%A1%B9%E7%9B%AE%E5%AE%A1%E6%A0%B8%E8%90%BD%E5%9C%B0%E5%AE%9E%E7%8E%B0.png)



### 个人对于score，lable，suggest的理解⭐

在七牛中，对应的置信度（score）表示的是，**对于判定结果的肯定比率**：
前提：
【pupl 中的 黄暴区间为 [ 0.7 ,0.99 ] ---- > lable为'非normal'的时候，如果score在这个区间内，则标记为pupl 】

例如：pulp 中，对于某个判定为：{"label" : "normal" , "score":"0.78", "suggesstion" : "pass" }，表示为：该视频得出结果是"normal"的，并且，他是通过了78%的认可的出来的；所以，最终得出的结果是"Pass";

再有：pulp 中，对于某个判定为：{"label" : "normal" , "score":"0.508", "suggesstion" : "review" }，表示为：该视频得出结果是"normal"的，并且，他是通过了50.8%的认可的出来的, 认可率较低；所以，最终得出的结果是需要人工审核。

再有：pulp 中，对于某个判定为：{"label" : "pulp" , "score":"0.508", "suggesstion" : "review" }，表示为：该视频得出结果是"pulp"的，并且，他是通过了50.8%的认可的出来的, 认可率较低；所以，最终得出的结果也是需要人工审核。

反例：例如：pulp 中，对于某个判定为：{"label" : "pulp" , "score":"0.78", "suggesstion" : "block" }，表示为：该视频得出结果是"pulp"的，并且，他是通过了78%的认可的出来的；所以，最终得出的结果是"block";

 

综上所属，**score是表示对于label的肯定度**。
而该score，我们是可以通过后端来变更的，也就是自定义置信区间，从而使得相同的label、相同的score得出不同的suggestion。

![debug审核结果](assets/debug审核结果.jpg)







### 审核返回 json 结果

#### 文本审核七牛返回结果

[API调用文本审核_API 文档_内容审核 - 七牛开发者中心](https://developer.qiniu.com/censor/7260/api-text-censor)

```java
{
    "message": "OK",
    "code": 200,
    "result": {
        "scenes": {
            "antispam": {
                "details": [
                    {
                        "score": 0.99985,
                        "label": "normal"
                    }
                ],
                "suggestion": "pass"
            }
        },
        "suggestion": "pass"
    }
}
```







#### 图片审核七牛返回结果

[API调用图片审核_API 文档_内容审核 - 七牛开发者中心](https://developer.qiniu.com/censor/5588/image-censor)

```java
{
    "message": "OK",
    "code": 200,
    "result": {
        "scenes": {
            "terror": {
                "details": [
                    {
                        "score": 0.99985,
                        "suggestion": "pass",
                        "label": "normal"
                    }
                ],
                "suggestion": "pass"
            },
            "politician": {
                "suggestion": "pass"
            },
            "pulp": {
                "details": [
                    {
                        "score": 0.65191,
                        "suggestion": "pass",
                        "label": "normal"
                    }
                ],
                "suggestion": "pass"
            }
        },
        "suggestion": "pass"
    }
}
```

#### 视频审核返回结果

[API调用视频审核_API 文档_内容审核 - 七牛开发者中心](https://developer.qiniu.com/censor/5620/video-censor)

```java
{
    "id":"5c416b7afb09fe00089e37be",
	"vid":"video_censor_test",
    "request":{
        ...
    },
    "status":"FINISHED",
    "result":{
        "code":200,
        "message":"OK",
        "result":{
            "suggestion":"pass",
            "scenes":{
                "politician":{
                    "cuts":[
                        {
                            "offset":0,
                            "suggestion":"pass"
                        },
                        {
                            "offset":657,
                            "suggestion":"pass"
                        }
                    ],
                    "suggestion":"pass"
                },
                "pulp":{
                    "cuts":[
                        {
                            "details":[
                                {
                                    "label":"normal",
                                    "score":0.99976,
                                    "suggestion":"pass"
                                }
                            ],
                            "offset":0,
                            "suggestion":"pass"
                        },
                        {
                            "details":[
                                {
                                    "label":"normal",
                                    "score":0.99998,
                                    "suggestion":"pass"
                                }
                            ],
                            "offset":657,
                            "suggestion":"pass"
                        }
                    ],
                    "suggestion":"pass"
                },
                "terror":{
                    "cuts":[
                        {
                            "details":[
                                {
                                    "label":"normal",
                                    "score":0.76111,
                                    "suggestion":"pass"
                                }
                            ],
                            "offset":0,
                            "suggestion":"pass"
                        },
                        {
                            "details":[
                                {
                                    "label":"normal",
                                    "score":0.52299,
                                    "suggestion":"pass"
                                }
                            ],
                            "offset":657,
                            "suggestion":"pass"
                        }
                    ],
                    "suggestion":"pass"
                }
            }
        }
    },
    "created_at":"2019-01-18T14:00:26.865+08:00",
    "updated_at":"2019-01-18T14:01:01.148+08:00",
    "rescheduled_at":"2019-01-18T14:00:26.865+08:00"
}
```







## 用户模块 - 个人信息

**CRUD**为主

在使用ThreadLocal时候，向外暴漏的接口必须强制在形参中传参，禁止在方法内直接凭空 ThreadLocal.get()。不然会导致方法意义不明确。在项目中未能遵循。
例如：
VideoServicer#deleteVideo(Long id) 中只写了 videoId，实际应该把 userId 也一并传入。
在 Controller 层面不用理会，因为 Controller 不会向外暴漏，而是交给 web 框架调用



- 必须有默认收藏夹：修改用户信息时，需要检验用户修改的默认收藏夹，对应的收藏夹是否存在





## 用户模块 - 关注

follow表设置联合唯一索引：UNIQUE KEY `index_user_id_follow_id` (`user_id`,`follow_id`) USING BTREE

redis的作用是为了在之后的“**共同好友**”，“互关”操作中，更加方便的操作才引入的

关注数可能不会很大，但是粉丝数会大的恐怖

使用Zset，用时间戳当score，自己的关注Zset，对方的粉丝Zset都要添加

Zset有start和end可以范围获取，实现分页



### 怎么看互关

User类中有一个字段each，返回粉丝/关注列表时，用一个hashSet先存粉丝列表，然后再得到一个关注列表，再new出一个map，key是粉丝id，value是是否互关，最后把他一个个查出User信息并修改each信息返回



### Redis存储ttl设置

每次用户登录时，把关注，粉丝列表查出来放在 Redis 里，设置一个2小时的 ttl









## 用户模块 - 收藏夹

CRUD

两张表：favorite，favorite_video



**用户id + 收藏夹名称**做一个联合唯一索引

所有收藏夹默认是私有状态，不能给别人看

在查看收藏夹时，需要使用 jwt 获取用户id检验是不是本人在查看收藏夹，然后才能根据收藏夹id查数据库





## 接口校验参数

写的接口可以理解成 Controller 接口也可以理解成 Service 层接口

在Service层做参数校验（可能会暴露接口给别的服务使用），判断上层传来的参数对不对，是不是null

```java
throw new BaseException("不存在的分类");

BaseException extends RuntimeException
```

抛出一个**运行时异常**

如果抛的是**Exception（编译时异常）**，则需要**try-catch或者向上抛**，又因为我们的每一个Service层接口都需要参数校验，每一个方法都在后面写一个throw Exception显然不太合理，所以需要抛出一个运行时异常

最后通过**全局异常处理器 @ExceptionHandler(BaseException.class)** 捕捉然后返回





## 推送模块 - 兴趣推送⭐：标签推送

我们的兴趣推送是基于标签概率算法推送的，我们有一个用户模型，数据结构是一个map，key是标签，value是对应标签的概率（double分值，并不是0~1百分比），用户来到我们的网站，需要订阅分类，每个分类背后会有多个默认的标签；用户订阅了某些分类后，相当于订阅了某些标签，我们需要初始化用户模型，计算出他的概率值，用100除以当前的标签数，得到他们对应的概率，如果初始化的时候有3个标签那么就是33.333；

兴趣推送时，我们首先将用户模型从Redis里获取，把Map转换成一个数组，将所有的概率加起来是数组长度，整个数组上有不同标签概率的区间，小猫和足球，各概率是50，所有0-49是小猫标签，50-100是小狗标签的id。

再取随机数，默认返回十条视频，概率越大说明占用数组长度越大，取完以后，lableid会有多个，从标签库里获取视频是一个随机行为，mysql很难做随机行为，存在Redis里，因为有八个，所以我们用管道传递一批数，返回一系列视频id，视频可能用户看过，再用布隆过滤器去一次重，我们还会再随机推送一条不相关的视频，防止用户一直刷不到某些标签的视频（也可能是热门视频）

这时候前端可能获取了四五条视频，但是前端并不是看完所有视频才请求下一次的，到达一个阈值比如70%就获取下一次的视频。

用户对视频的行为会修改用户模型的概率，当然修改用户模型是一个很频繁的操作，所以我们用了一个请求合并，堆积修改请求，之后一并发给后端

为了防止概率膨胀，我们会在达到阈值时，同比例缩放概率



- 订阅分类
- 用户模型设计
- 根据用户模型推送视频
- 用户模型概率增减少
- 请求合并（没写）
- redis管道
- 布隆过滤器（没写）



兴趣推送是通过标签相似推送算法实现。

前提：
1个分类对应 n 标签，n 个标签对应 n 个视频
用户模型：
每个用户拥有用户模型，存储 redis 中，用户模型是 Map 结构， key：标签id，value：对应的概率，ttl：30天

 

> Q：用户模型如何初始化

A：新用户在注册时会选择订阅分类，订阅的分类其实是订阅分类后的标签。假设订阅了体育和宠物分类，该两个分类对应的标签为：体育，宠物。则用户的用户模型为：
{
体育：50
宠物：50
}

> Q：如何兴趣推送

A：用户：从 redis 中获取用户模型，将概率组装为数组，对应的下标上存储标签 id。在该数组上进行随机数获取标签id，再通过标签库随机获取视频。获取完一批视频后，对视频进行**布隆过滤器**排除推送过的视频。为了用户能刷到其他视频，人为干扰通过用户的**性别**选择推送其他视频。
A：游客：随机获取10个标签，从标签库中随机获取视频，不用在乎去重问题。

> Q：用户模型如何计算

A：用户模型会根据用户在观看视频中的操作进行计算：用户点赞，收藏，分享等正向行为，会给用户模型添加对应的概率。添加方式为：正向操作的视频地下所有的标签加上概率。负向操作为：停留当前视频小于 x 秒（不感兴趣这一类视频）因此用户模型减少。为了防止用户模型概率的膨胀（一直正向操作，导致对应值越来越大，最后计算会出问题）因此每次计算模型后会同步增长减少概率值。计算方式为：每个标签概率同等加上标签数，再同等除以标签数 。

> Q：用户对视频的操作概率添加减少如何设置

A：点赞，收藏，浏览，评论，分享，观看都属于正向行为，每种行为对应的分值不一样，需要手动设置。

> Q：用户模型的更改涉及到大量请求，如何优化?

A：可将请求进行合并，前端可做合并，后端也可对在一定时间节点内的修改用户模型进行请求合并。
实现： 在一定的时间内堆积请求，将参数进行合并，到时间后一并发送。例如：
后端：
接收请求后放入队列，通过一个定时任务的线程每隔200ms从队列中取出元素对相同的用户模型进行合并后处理

浏览对模型的影响使用合并请求

点赞和收藏单独进行标签修改

---

Redis中会存每个标签下的视频ids（发布时存入），用于在推送时查视频id



### 用户模型

用户模型 user:model:17  key:lable名（String），value:概率值，double，不仅仅是0-1，可能是几十可能是几百可能是几个

![1734685775108](assets/1734685775108.png)

![1734687109726](assets/1734687109726.png)



### 布隆过滤器实现过滤看过的视频

给每个用户建一个看过视频的布隆过滤器，推荐系统推送的内容使用布隆过滤器过滤一下，把不在列表里的让客户可见即可；

最佳的原始数据和位图位数比是**1:20**，经过**8次hash**，误判率会在千分之一左右。如果把hash次数提高，误判率会更低。

定期重构布隆过滤器

定期删除很久之前看过的视频





## 推送模块 - 分类推送

![1734706340140](assets/1734706340140.png)

### 设计

A：调研了其他短视频平台的分类推送发现核心是随机推送，并且分类推送接口会被频繁调用，以及mysql无法实现随机获取视频，因此采用 redis 的 set 结构进行存储视频。又为了考虑到视频数据量会很大，此操作会造成大k，因此将结构改为分片存储，这里涉及到 ID分发器(后续会讲)。

在业务上随机推送可能会推送到古老视频，为了视频的实时性。我们可通过分片存储的 ID 保证推送的视频大部分为新视频。假设：最新 ID 为9，则说明一个分类对应视频分片成了9个块，将其分为1-6,7-9 两个区域，前者在区间取3个随机数，后者取7个随机数。分别从对应结构中随机获取视频，因此该场景问题。

 

### 分片

因分类推送是用 Redis 的数据结构进行存储，并且分类下会有大量视频，为了避免大 key（一个key下对应很多数据）。因此采用分片思想（将一块数据分成多块）进程存储。

原始分类 key：pet：videoId1、videoId2...
改造后分类 key：pet:1：videoId1、videoId2...
改造后分类 key：pet:2：videoId1、videoId2...
...
pet:1后的1称之为 ID
改造后的分类对应的视频最多存储 5000 条，超过 5000条，则 ID ++，新创建一个分类:ID进行存储。

> Q：ID 如何生成?

A：通过 Redis 管理，每一个分类对应当前 ID，数据结构：string，key：pet ，value：0。每次在分类推送前需要先组装分类key：分类 : ID，也就是每次都要先获取分类对应的 ID。每次发布时候后都要判断分类下的视频是否大于 5000 条。优化点：获取分类对应的 ID 和 分类获取随机视频是两个行为，但是可以**组装成 lua 脚本节省网络开销**

> Q：分类 + ID 是如何解决分类推送可能会推送老视频？

A：ID越老，说明视频越老。
假设现在有一个宠物分类，当前 ID 为10，分类推送计算方式如下：
1.将 ID 分为7 3 区域，3为老视频 对应 ID 分类的区域，7为新
2.对于 1-7 老视频随机获取 3条视频，对于 8-10新视频随机获取7条视频

这样既能做到新视频的推送也能不丢弃老视频。这里的算法也可自行调整



### 对其他模块的影响

视频上传，在数据库中要冗余对应分类的自增id

分类id自增器可以使用本地缓存存储，如果是分布式必须使用redis存

ID分类器自增的时候，需要将Redis中前面的几个分类key删除







## 推送模块 - 热门视频推送 & 热度排行榜

> diss：在网上大家可能百度到的热度排行榜大概率都是使用 redis 的 zset 来实现，但是并没有考虑到核心的是排行榜需要更新 -> 视频热度的变化

### 热度计算方式

**发布时间越短：从视频发布至当前时间**
**视频互动率：视频点赞，评论，收藏，分享，观看时长，等行为**

我们不难想出的是：视频的热度是会随着视频发布的时间逐步降低。并且在两个视频同等的点赞，但是发布后的时长不一样，而导致发布时间越短的视频热度更高。因此我们可以得出一个公式：

> 视频互动率 / (现在时间 - 视频发布时间) = 视频的热度

 

这样一个简单的热度计算公式即可算出。

但是热度的衰减也是需要控制的，因此这里引出半衰期公式。

具体半衰期公式咋搞的，咱也不会，懂的可以评论区留言。

<img src="assets/1734708121105.png" alt="1734708121105" style="zoom: 67%;" />

#### 经典半衰期公式

给大家讲一下半衰期公式，这是物理上的一个概念，我们对半衰期公式的第一个印象一定是半衰，对于积分100来说（项目里的weight），经过某固定长度的时间（例如经过一小时），积分对半衰减

- 0小时，100积分
- 1小时，50积分
- 2小时，25积分
- ...以此类推

所以，经典的半衰期公式就应该是 : **某个固定量 / (2的t次方)**
假设固定量为x，衰减时间为t，则可以理解为：**x \* 2的(-t次方)**
当然，理解的时候需要把t想象为**离散的整数**，后续我们再理解，他的实际情况其实 t 是一个**连续的double**

#### 变化曲线特性(便于理解为什么能用半衰期公式计算热度)

热度是一个随着时间变化很快的东西，半衰期的曲线图正好是一个刚开始衰减很快，到后来逐渐放缓的过程。
我们来设想一下如果直接使用热度除以时间的计算公式，如果一个视频的点赞收藏浏览等等对权重有影响的操作每时每刻的强度都是一样的，那么这个视频的热度将会永远不变，（时间衰减了，weight增加了，正好互补）而我们希望的是，时间特别久了以后，热度不应该再起来了，半衰期公式正好呈现了一个**指数**级别的衰减

#### 项目中的半衰期公式解读

**视频热度 = 互动率 × Math.exp（-a × 距今发布时间 ）**
半衰期是每次衰减一半，那么我们可不可以通过调整衰减程度来调整公式呢，当然可以，比如每次衰减为原来的三分之二，或者衰减为原来的三分之一，都能修改半衰期公式
对比原来的公式，我们把：x * 2的(-t次方)，中的2换成了自然常数2.718..，也就是说由每次衰减为原来的一半，变为了衰减为原来的0.37（1/e）
这里的a乘上时间，其实是为了来控制半衰时间（衰成0.37也可以成为一次半衰），假设a为1，发布时间单位为小时，则每一小时热度就会半衰一次，但是如果a为2，则变成了一小时半衰两次，a为0.5，就变成了两小时半衰一次。
这样大概可以理解半衰期公式了

#### 问题所在

**重点理解：半衰时间，半衰一次的衰减程度**
**重点区别：半衰时间，多久修改一次热度**
假设定时任务是1小时计算一次热度，但是这个1小时，不等于半衰时间，半衰时间应该由a来进行控制，实际上热度应该是每分每秒都在变化的，只不过我们间隔1小时才看一次而已
问题所在：如果半衰时间设置的很短，比如设为了3小时，（便于计算假设半衰程度是2，不是e，e算起来太麻烦了）意味着一个weight为800的视频在发布12小时后热度为50，但是一个weight为100的视频在发布3小时后热度也为50，我认为这显然不太合理。半衰时间设置为12小时到一天可能比较合理（个人认为）





### 热度排行榜

要明白的是：怎样都要全表扫描，如何**优化全表扫描**的过程才是重点

**排行榜 = topK 问题**

- 根据**主键分页**查询，可以极大提高查询效率
  分页查询会涉及到 offset 和 limit，假设 offset 为10w，limit为10，在普通分页下扫描 offset + limit 行数，并且丢弃 offset，取 limit
  主键分页查询根据索引直接定位到 id 后面，并且直接取 limit 即可。而主键的获取可以根据上一次查询最后一条记录得到
- **每次主键分页查询出来数据后在内存当中做 TopK 问题**
- 最后查出来的数据再放入 Redis 的 zset 中即可
  存放 Redis zset 中直接插入即可。插入完后，删除10条以后的数据

**优化：**
目前是单线程查找数据，可**优化为多线程**：

- **查询视频数量**
- 根据视频数量**进行切片**，分为 n 个线程，每个线程负责一个块，例如：10w 条数据，每 1w 条切片，10个线程，线程分别负责 1~ 10w，10 ~ 20，以此。
- 后续的逻辑不变

 

**topK 问题**
topK 指的是在大量数据中找到前 N 条数据，实现方法可用**小根堆**实现，小根堆底层数据结构是二叉树，并且**树顶保存的是最小值**，这样，**在查看当前数是否大于小根堆中的元素时**，可获取小根堆顶部元素即可知道是否大于堆(排行榜)中的元素。

为什么不能用大根堆实现？
**大根堆顶部保存的是最大的值。**
现在要插入一个比最大值小的值，没有办法查询，除非挨个遍历。

> 大量数据查询计算，都可以采用该方案：**主键分页 + 切片 + 多线程**。甚至是可以用到 Fork Join

**Fork Join**
将大量数据切片后进行计算，并且有线程计算完成后，会抢其他线程的数据。而普通的切片，A线程执行完后，并不会抢其他线程的数据。 Fork Join 的目的是为了尽可能压榨多的 cpu。

**使用：**
stream 流中的并行流就用到了 Fork Join，或者手动 new ForkJoinPool

```java
new ArrayList<>().stream().parallel().
```



> - 写个脚本，创建100w条数据，分别通过这俩种方式查询数据，哪个更快?为什么?分析explain。
> - 因为技术需求，id一定要有序，除了自增之外，还有什么方式？



### 热门视频推送

查询出来以后进行存到Redis里，Redis的key分天存储ttl设个3天到时候自己没了就好

推送的时候要从最近三天里选视频推送，今天的推送10条热门视频，昨天3条，前天2条，放在一个set里

热门视频要不要查重？可用可不用，因有布隆过滤器查重很快



- 根据`id`分片，查询过去三天内的视频，每次1000条
- 筛选出热门视频，hotVideos
- 更新查询的起始 `id` 为当前查询结果中最后一个视频的 `id`，实现分页，继续查询下一批视频
- 将热门视频存入Redis中，key用今天的日期，ttl为3天

问题：redis的一个set中，存的不止今天的热门视频，还有过去两天的热门视频

最后热门推送的时候，取出的视频id需要放在set里，然后再数据库读视频详细信息。

可以在推送时和用户看过的视频做一个去重（布隆过滤器）







## 推送模块 - 相似视频推送

根据视频**标签**进行推送

查出视频的标签，然后随机选择一个标签，去Redis中，查出该标签下最近的视频，然后随机选择 10 个推荐

排着去重，布隆过滤器，只要不重就直接返回，因相似视频一次只推送一个，如果业务改成一次推送好多个也可以







## 推送模块 - Feed 流

推拉结合

**滚动分页**，没有通过下标的方式去拉数据



<img src="assets/1734776794423.png" alt="1734776794423" style="zoom:80%;" />

关注推送属于 feed 流场景：推拉模式，场景：关注，粉丝

推模式

> xhyovo 发布了一条视频后，主动推送到其粉丝下的动态中，粉丝上线后即可观看 xhyovo 的视频。但是如果粉丝量很大呢？发布一条视频后，都要推给其下的粉丝。

拉模式

> xhyovo 发布了一条视频后，并不主动推送到其粉丝下，而是粉丝上线后主动拉取关注的用户新发布的视频。但是如果该用户关注了很多 up 主呢？而这些 up 主在这些期间又发布了很多视频呢？

问题
推模式会导致用户发布视频后，如果粉丝量太大，会导致推送大量视频，其中不乏僵尸粉、不活跃的粉丝等
拉模式会导致用户长时间不上线后，进而上线拉取大量视频

解决：
**推拉模式结合**
每个用户拥有收件箱和发件箱
收件箱：接收关注的人发布的视频 redis ：zset： ttl：7 天
发件箱：每个用户自己发布的视频 redis：zset：ttl：-1

用户发布一条视频后，主动推送到在线的用户的收件箱中，并且 ttl 为7天。用户上线后，主动拉取发件箱中最新一条视频至当前时间之内的关注的人发件箱的视频。如果发件箱无数据，则拉取关注人发件箱中的 7 天之内的视频(redis的zset api 支持)。7天之外的视频则属于**冷视频**，可从 db 中获取，因为看到这里的用户并没有很多。而 7 天之内的数据一般足以用户观看。



- 在线/离线
  通过心跳包检测即可。
  假设心跳包5秒，用redis存储，ttl为10秒，则每5秒续签。mysql字段存储，因为没有ttl，则可以用上一次活跃时间存储，在后续查询的时候通过当前时间和用户的上一次活跃时间的差值就可以计算是否在线，如果大于10s，说明已经离线。

问题是如何将在线用户和关注推送中的推模式结合起来，现在的逻辑：查出所有的粉丝，筛选出在线粉丝，那如果粉丝很多就g了。所以可以在关注粉丝表中存储一个冗余字段（是否在线）即可



### 总结输出

**在线推，离线拉**

使用推拉结合的方式实现关注推送，每个用户在Redis中都会有一个发件箱和收件箱，使用Zset，如果Redis中没有，在用户登录时，需要异步创建，ttl为14天，箱中需要有一个默认元素，nomore来确定这个发件箱里是空的，而不是没有箱子。

> 当 Redis 中没有某个 Timeline 的缓存时我们无法判断是缓存失效了，还是这个用户的 Timeline 本来就是空的。我们只能通过读取 MySQL 中的数据才能进行判断，这就是经典的缓存穿透问题。
>
> 对于时间线这种集合式的还存在第二类缓存穿透问题，正如我们刚刚提到的 Redis 中通常只存储最近一段时间的 Timeline，当我们读完了 Redis 中的数据之后无法判断数据库中是否还有更旧的数据。
>
> 这两类问题的解决方案是一样的，我们可以在 SortedSet 中放一个 `NoMore` 的标志，表示数据库中没有更多数据了。对于 Timeline 本来为空的用户来说，他们的 SortedSet 中只有一个 NoMore 标志：防止缓存击穿

用户发送视频时，看他的粉丝在Redis里有没有收件箱，如果有则发送给他们，并且检查粉丝收件箱超过了14天的视频，把他们删除。用户自己的发件箱ttl设为14天，发送的时候往发件箱发一条数据，并且删除掉14天以前的视频

用户上线时，如果有收件箱，证明最近用户一直在线，不需要拉任何视频

用户上线时，如果没有收件箱，则从关注列个拉取最近 14 天的视频放入收件箱





## Feed流

[从小白到架构师(4): Feed 流系统实战 - -Finley- - 博客园](https://www.cnblogs.com/Finley/p/16857008.html)

拉模型，读扩散

推模型，写扩散

推模型的逻辑要复杂很多，不仅发布新文章时需要实现相关逻辑，新增关注或者取消关注时也要各自实现相应的逻辑，听上去就要加很多班。。

![1734861074134](assets/推拉模式比较.png)

虽然乍看上去拉模型优点多多，但是 Feed 流是一个极度读写不平衡的场景，读请求数比写请求数高两个数量级也不罕见，这使得拉模型消耗的 CPU 等资源反而更高。

此外推送可以慢慢进行，但是用户很难容忍打开页面时需要等待很长时间才能看到内容（很长：指等一秒钟就觉得卡）。 因此拉模型读取效率低下的缺点使得它的应用受到了极大限制。

我们回过头来看困扰推模型的这个问题「粉丝数多的时候会是灾难」，我们真的需要将文章推送给作者的每一位粉丝吗？

仔细想想这也没有必要，我们知道粉丝群体中活跃用户是有限的，我们完全可以只推送给活跃粉丝，不给那些已经几个月没有启动 App 的用户推送新文章。

至于不活跃的用户，在他们回归后使用拉模型重新构建一下关注 Timeline 就好了。因为不活跃用户回归是一个频率很低的事件，我们有充足的计算资源使用拉模型进行计算。

因为活跃用户和不活跃用户常常被叫做「在线用户」和「离线用户」，所以这种通过推拉结合处理头部作者发布内容的方式也被称为「**在线推，离线拉**」。



使用「在线推，离线拉」策略时我们需要判断用户是否在线，在为 Timeline 设置了过期时间后，Timeline 缓存是否存在本身即可以作为用户是否在线的标志。











## 视频模块 - 上传视频

![1734795045989](assets/1734795045989.png)

A：视频在进入审核逻辑后已经发布成功，只是视频状态为审核中，因为后续流程不涉及视频发布主流程

> Q：视频源以及封面为什么不能修#1587px #929px改

A：是为了防止视频热度太高后用户自己更换视频源信息，挂羊头卖狗肉。别的平台可以是因为人家有策略能校验

> Q：审核队列

A：在审核视频中会有当前审核状态：快/慢审核，可根据规则引擎(用户的粉丝度 / 用户等级) 决定快慢审核。慢审核状态代表线程池已开启队列。当前状态为慢审核用户选择快审核逻辑： 再开辟一个线程即可

> Q：审核机制如何实现？

A：审核是通过调用七牛云视频/封面/文本审核API，对方会提供审核建议以及得分。我们这里是自主管控审核力度，只需要审核后的得分。根据后台策略分值区间（审核通过、人工、PASS）决定走对应的逻辑







## 视频模块 - 浏览记录⭐

Redis Zset，冗余存储一些信息

zset 每一次插入时，删除超过阈值 14 天以前的

```redis
ZREMRANGEBYSCORE key min_score max_score
```



![1734766111630](assets/1734766111630.png)

### 布隆过滤器去重

布隆过滤器无法实现删除，只能增 查

在Redis中实现布隆过滤器

设置一个ttl，和浏览记录zset里元素存在时间一样，用户登录的时候，如果没有就用浏览记录做一次重构，如果有则无需理会

浏览记录 zset 的 ttl：1个月，元素 remove 时间为1个月

布隆过滤器 ttl ：1个月





## 视频模块 - 点赞

点赞 取消点赞 设置为一个接口

查询是否点赞，点过就取消，没点过就点

需要加一把synchronized锁，锁住用户id和视频id拼接在一起的String intern()

点赞后修改用户模型,取消点赞也需要修改用户模型

视频热度不需要管



### 点赞并发

点赞的更新是一个 cas 思想，但是这里没有做重试

```java
final UpdateWrapper<Video> updateWrapper = new UpdateWrapper<>();
updateWrapper.setSql("start_count = start_count + " + value);
updateWrapper.lambda().eq(Video::getId, video.getId()).eq(Video::getStartCount, video.getStartCount());
update(video, updateWrapper);
```



## 视频模块 - 评论

- 一二级评论
- 树级评论

**API**

- 查看评论
- 打开子级评论
- 发布评论
- 删除评论



### 需求分析

在项目中很大概率会有评论模块，这是用户与TA人之间沟通的桥梁，例如：社区、短视频、贴吧等。
但是如何设计一个好的评论模块成了最大的问题：

> 评论涉及到父子评论关系，是一个很典型的树形结构，那在查询评论时是否带有树型结构？
> 删除评论时，是否将该评论下的子评论也一并删除？

 

> tips
**评论不涉及修改**，因为修改评论会让评论有争议存在

 

### 方案1

**树形评论**

在评论中会涉及到**一级评论以及子评论**，每条评论拥有 parent_id(该评论的父评论id)

sql设计

```sql
CREATE TABLE `comments` (
  `id` int(11) NOT NULL AUTO_INCREMENT, // 主键
  `parent_id` int(11) NOT NULL, // 父评论id，如果为0则是一级评论
  `content` longtext NOT NULL, // 内容
  `from_user_id` int(11) NOT NULL, // 评论人
  `to_user_id` int(11) DEFAULT NULL, // 被回复人
  `business_id` int(11) NOT NULL, // 对应业务id
  `deleted_at` datetime DEFAULT NULL, // 逻辑删除
  `created_at` datetime DEFAULT NULL, // 创建时间
  PRIMARY KEY (`id`),
  KEY `root_id_index` (`root_id`)
) ENGINE=InnoDB AUTO_INCREMENT=237 DEFAULT CHARSET=utf8;
```

因此，展示结构如下：
A 评论了文章
-------B 评论了 A
-------------C 评论了 B
---------------------D 评论了 C

#### 问题

查询评论下的子评论需要**递归查询**，要么在 db 层面做递归，要么在应用层面做递归，如果评论的楼层有几百层呢？效率太低了

查询一条评论流程如下：

- 查出该评论
- 根据父评论找到二级评论
- 根据二级评论找三级评论
- ....

伪代码

```java
{
List<Comment> childComments = CommentService.getChildCommentsById(8);
findChildComment(childComments,8);
}

public void findChildComment(List<Comment> comments,int parentId){
	for(Comment comment:comments){
		if(comment.id == parentId){
			comment.getChildComments().add(comment);
			findChildComment(comments,comment.id)
		}
	}
}
```

删除评论也是同理：找到删除评论下的所有评论一并删除

#### 优势

- 想查看某条评论下的所有评论的时候结构很清晰，清楚的知道评论结构

### 方案2

**将树形结构分为一二级结构**

为了解决树形设计中递归的问题，我们可以将树形结构转为**只有一级评论和二级评论**。这样做的好处是可以不用递归查询，坏处是如果想知道该评论下的评论情况，就只能往后翻页。在删除评论的时候如果不是一级评论则只用删除该评论即可，如果删除的是一级评论，则将子评论一并删除。

sql设计
加上 root_id 字段，用于区分二级评论的一级评论是谁

```sql
CREATE TABLE `comments` (
  `id` int(11) NOT NULL AUTO_INCREMENT, // 主键
  `parent_id` int(11) NOT NULL, // 父评论id，如果为0则是一级评论
  `root_id` int(11) NOT NULL, // 一级评论id，如果为0则是一级评论
  `content` longtext NOT NULL, // 内容
  `from_user_id` int(11) NOT NULL, // 评论人
  `to_user_id` int(11) DEFAULT NULL, // 被回复人
  `business_id` int(11) NOT NULL, // 对应业务id
  `deleted_at` datetime DEFAULT NULL, // 逻辑删除
  `created_at` datetime DEFAULT NULL, // 创建时间
  PRIMARY KEY (`id`),
  KEY `root_id_index` (`root_id`)
) ENGINE=InnoDB AUTO_INCREMENT=237 DEFAULT CHARSET=utf8;
```

回复评论：
评论

- 将 parent_id 和 root_id 设置为该评论主键id

回复评论

- 将 parent_id 设置为回复的评论，root_id 设置为回复评论的 root_id

查询评论：
因现在的评论结构只有一二级评论，因此只会查询一级评论

- 根据评论id查询 root_id
- 根据 root_id 查询该 root_id 下的所有评论
- 时间排序分页处理

删除评论：

- root_id == id 说明是一级评论，则删除该评论下的所有子评论
- root_id != id 则删除该评论即可

 

查询文章下的一级评论并且携带二级评论：

- 根据分页查出该文章下的一级评论的 root_id

- 将 root_id 带入以下sql

  ```sql
  select c.* from comments c where (select count(id) from comments where root_Id = c.root_id and id<=c.id ) <= 5 and  c.root_id in  ? order by root_id desc
  ```


> tips
不同的场景选择不同的设计方案
方案1：清晰的楼层结构，性能虽然有所损耗，但是对于用户友好
方案2：性能友好，但是对于用户阅读及其不友好，可以从b站的评论区看出





## 后台模块 - RBAC（没整透，感觉不是重点）

五张表

AOP实现环绕通知

@Authority



## 补充

- 获取视频详情

视频存储在Redis中，点赞，收藏等行为异步放入DB，定期扫描DB中不重要更新Redis

- Async使用
- 邮箱smtp使用限制

发送文件大小不能超过20M，放oss，给下载地址，让他们下载就好了

- Service层设计，如何避免循环依赖

Service层返回的结果一定要最小化，最好不要直接返回其他Service的entity

比如在UserService下要返回视频相关内容则只返回视频id，但是他可以返回User类的详细信息

- Aware接口，Spring下的一个知识，学Spring时复习







## 标签推送改造（必看）⭐

项目地址：https://gitee.com/XhyQAQ/smart-label-push

### 背景

目前项目的兴趣推送是基于标签推送，而标签的匹配和存量是通过精准匹配，而会造成某个问题，例如：张三观看了宠物视频，宠物标签是：拉布拉多。那其实张三可能对狗或者猫都感兴趣。而目前的设计只会推送 拉布拉多 标签的视频

**因此此次改造就是为了解决这个问题**

一维数组：向量维度：1536
拉布拉多：[1,2,3...]
金毛：[1,2,4...]

### 改造计划

- 引入向量化
- https://help.aliyun.com/zh/model-studio/developer-reference/model-introduction-6?spm=a2c4g.11186623.0.0.34f925b39JoWQ7
- 1.获取 apikey
- 2.引入 sdk
- 3.cv代码
- run

 

- 引入 LLM
- 向量数据库

通过以上俩点改造标签的存量

### 概念

数据向量化：将数据给向量化
向量化数据库：用于存储向量化数据，并且提供 API 接口
LLM：大模型

 

流程

- 用户发布视频时将标签存储到向量化数据库中
- 用户对该视频感兴趣时，将视频标签进行向量化查询
- 查询到的数据交给 LLM 进行过滤
- 过滤：根据视频的标签和搜索出的标签，选择和视频标签最相关的标签
- 例如（prompt）：我将给你俩组标签，请筛选出第二组的标签，筛选规则是是第二组的标签能和第一组标签进行相似匹配，尽可能多的匹配，并且按照特征进行匹配，你可以将第一组的标签提取出特征，在第二组和第一组匹配的是按照特征进行匹配。

 

- 将获取的标签继续存量并且计算（这里的逻辑和之前一致）

 

### 安装 postgre

使用 postgre 的 vector 作为向量化数据库
https://github.com/pgvector/pgvector

#### 安装 vector 拓展

查看 verctor 拓展

创建数据库输入以下命令：
SELECT * FROM pg_extension WHERE extname = 'vector';

如果没有则拉取拓展：
sudo apt install postgresql-<version>-pgvector

如果没有 apt 命令，则通过源码方式安装（这一步问 gpt，可能会遇到很多问题，例如：没有 make 命令，没有配置 pg 环境变量）

总之到最后：
安装拓展：CREATE EXTENSION vector;
查看拓展：SELECT * FROM pg_extension WHERE extname = 'vector';

能正常运行显示即可

### 数据库表设计

#### label表

```sql
create table public.label (
  id integer primary key not null default nextval('label_id_seq'::regclass),
  name character varying(255) not null,
  vector vector(1536)
);
```

### 相似性度量方法

#### 内积（Inner Product）

##### 使用场景

- **推荐系统**：
- **商品推荐**：通过计算用户和商品的嵌入向量的内积，评估用户对商品的偏好程度，从而推荐相似度高的商品。

 

- **自然语言处理（NLP）**：
- **词嵌入相似性**：在词嵌入模型（如 Word2Vec）中，内积用于衡量词向量之间的相似性，辅助词义相似性判断和文本匹配。

 

- **搜索引擎**：
- **文档相关性排序**：根据查询向量和文档向量的内积，评估文档与查询的相关性，从而进行结果排序。

 

- **深度学习中的相似性计算**：
- **特征匹配**：在深度学习模型中，用内积衡量不同层或不同样本之间的特征相似性，如在对比学习中用于计算样本之间的相似度。

 

##### 适用性分析

- **优点**：
- 计算效率高，适合大规模数据的快速相似性评估。
- 能够有效衡量向量的方向相似性，适用于推荐和匹配场景。

 

- **缺点**：
- 对向量的长度（模）敏感，不同长度的向量可能导致相似性评分的偏差。
- 不适用于需要忽略向量长度，仅关注方向的场景。

 

#### 余弦相似度（Cosine Similarity）

##### 使用场景

- **文本相似性与信息检索**：
- **文档相似度计算**：在基于 TF-IDF 或词嵌入的文本表示中，余弦相似度用于衡量文档之间的语义相似性，常用于搜索引擎的相关性排序。

 

- **推荐系统**：
- **用户与内容匹配**：通过计算用户和内容向量的余弦相似度，评估用户对内容的兴趣程度，进行个性化推荐。

 

- **自然语言处理（NLP）**：
- **句子与段落相似性**：用于衡量句子、段落或整个文档之间的语义相似性，支持文本分类、聚类和问答系统等任务。

 

- **生物信息学**：
- **基因序列相似性**：在基因序列比对中，余弦相似度用于评估不同基因序列的相似程度，辅助基因功能预测和进化分析。

 

- **图像检索**：
- **特征向量匹配**：在图像检索系统中，通过计算图像特征向量的余弦相似度，找到与查询图像在语义上相似的图像。

 

##### 适用性分析

- **优点**：
- 不受向量长度影响，仅关注向量方向，适用于高维、稀疏数据的相似性计算。
- 在高维空间中表现优异，适合文本、图像等复杂数据的相似性评估。

 

- **缺点**：
- 对于长度相近但方向不同的向量，可能无法准确反映实际的相似性。
- 计算过程中需要对向量进行归一化，增加了一定的计算开销。

 

##### 总结

这两种种相似性度量方法各有优劣，适用于不同的应用场景：

- **内积**适用于需要快速评估向量方向相似性的场景，如推荐系统和文本相关性排序，但对向量长度敏感。
- **余弦相似度**适合高维、稀疏数据的相似性计算，如文本相似性和图像特征匹配，能够忽略向量长度，仅关注方向。

















